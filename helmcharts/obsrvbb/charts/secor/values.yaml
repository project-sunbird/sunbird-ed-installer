# Azure Container Details
azure_account: "{{.Values.global.cloud_storage_access_key}}"
azure_secret: "{{ .Values.global.cloud_storage_secret_key }}"
azure_container_name: "{{ .Values.global.private_container_name}}"

s3_access_key: "cloud_private_storage_accountname"
s3_secret_id: "cloud_private_storage_secret"
s3_region: "cloud_private_storage_region"
s3_endpoint: "cloud_private_storage_endpoint"
s3_path_style_access: "s3_path_style_access"
s3_bucket_name: "{{ .Values.global.private_container_name}}"

storage_type: ""
storageClass: ""

image:
  pullSecrets: []

kafka: &kafka
  host: "kafka"
  port: "9092"

zookeeper: &zookeeper
  host: "zookeeper"
  port: "2181"

global:
  kafka: *kafka
  zookeeper: *zookeeper
  env: "dev"

gcp_bucket_name: "{{ .Values.global.private_container_name}}"

serviceAccount:
  create: false

base_config: &base_config
  enabled: true
  replicas: 1
  service_name: "raw_telemetry_backup"
  timestamp_key: "syncts"
  fallback_timestamp_key: "@timestamp"
  kafka_broker_host: "{{.Values.global.kafka.host}}"
  zookeeper_quorum: "{{.Values.global.zookeeper.host}}:{{.Values.global.zookeeper.port}}"
  max_file_size: "100000000"
  max_file_age: "14400"
  partition_prefix_enabled: "false"
  partition_prefix_key: "nil"
  partition_prefix_mapping: "{}"
  message_channel_identifier: "nil"
  output_file_pattern: "{partition}-{currentTimestamp}.json"
  message_parser: "com.pinterest.secor.parser.PatternDateMessageParser"
  storage:
    size: 10Gi
  requests:
    cpu: 500m
    memory: 500Mi
  lag_threshold_warning: 50000
  lag_threshold_critical: 100000

secor_jobs:
  telemetry-ingest-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/ingest"
    topic: "sunbird.telemetry.ingest"
    consumer_group: "sunbird-telemetry-ingest-backup"

  telemetry-failed-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/failed"
    topic: "{{ .Values.global.env }}.telemetry.failed"
    consumer_group: "{{ .Values.global.env }}-telemetry-failed-backup"

  telemetry-raw-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/raw"
    topic: "{{ .Values.global.env }}.telemetry.raw"
    consumer_group: "{{ .Values.global.env }}-telemetry-raw-backup"

  telemetry-unique-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/unique"
    topic: "{{ .Values.global.env }}.telemetry.unique"
    consumer_group: "{{ .Values.global.env }}-telemetry-unique-backup"  
   
  telemetry-denorm-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/denorm"
    topic: "{{ .Values.global.env }}.telemetry.denorm"
    consumer_group: "{{ .Values.global.env }}-telemetry-denorm-backup"

  telemetry-assess-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/assess"
    topic: "{{ .Values.global.env }}.telemetry.assess"
    consumer_group: "{{ .Values.global.env }}-telemetry-assess-backup" 

  telemetry-assess-failed-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/assess/failed"
    topic: "{{ .Values.global.env }}.telemetry.assess.failed"
    consumer_group: "{{ .Values.global.env }}-telemetry-assess-failed-backup" 

  telemetry-derived-backup:
    <<: *base_config
    base_path: "telemetry-data-store/telemetry/derived"
    topic: "sunbirddev.telemetry.derived"
    consumer_group: "sunbirddev-telemetry-derived-backup"   

  deviceprofile-backup:
    <<: *base_config
    base_path: "telemetry-data-store/deviceprofile"
    topic: "dev.events.deviceprofile"
    consumer_group: "dev.events.deviceprofile-backup" 

  druid-events-telemetry-backup:
    <<: *base_config
    base_path: "telemetry-data-store/druid/events/telemetry"
    topic: "{{ .Values.global.env }}.druid.events.telemetry"
    consumer_group: "{{ .Values.global.env }}-druid-events-telemetry-backup"   
  
  druid-events-summary-backup:
    <<: *base_config
    base_path: "telemetry-data-store/druid/events"
    topic: "{{ .Values.global.env }}.druid.events.summary"
    consumer_group: "{{ .Values.global.env }}-druid-events-summary-backup"

image:
  repository: sunbirded.azurecr.io/secor
  tag: 1.0.0-GA
  pullPolicy: IfNotPresent

exporter:
  image:
    repository: prom/statsd-exporter
    tag: latest
    pullPolicy: IfNotPresent

prometheus_rule_selector_app: prometheus-operator
prometheus_rule_selector_release: prometheus-operator

# If you enable this, secor lag alert rules will be created in the flink cluster.
# In our case the consumer group lag metrics available in core prometheus.
# So we need to create the secor lag alert rule in core prometheus.
# By adding this condition we are avoiding creating the secor lag alert rule in flink cluster.
alertrules:
  enabled: false

# This condition is whether to create the secor lag alert rule or not.
secor_alertrule:
  enabled: false


commonAnnotations:
    "helm.sh/hook-weight": "-5"