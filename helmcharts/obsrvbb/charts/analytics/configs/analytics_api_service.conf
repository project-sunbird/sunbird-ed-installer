application.env="{{ .Values.global.env }}"
    cloud_storage_client_id="{{ .Values.global.cloud_storage_project }}"              # project id
    cloud_storage_private_key_id="{{ .Values.global.cloud_storage_private_key_id }}"  # private key id
    cloud_storage_project_id="{{ .Values.global.cloud_storage_project }}" 
    cloud_storage_key="{{ .Values.global.cloud_storage_access_key }}"                # accountname
    cloud_storage_secret="{{ .Values.global.cloud_storage_secret_key }}"  
    spark.cassandra.connection.host="{{ .Values.global.cassandra.host }}"
    spark.cassandra.connection.port="{{ .Values.global.cassandra.port }}"
    # Data Exhaust API
    data_exhaust.list.limit="100"
    data_exhaust.retry.limit="3"
    data_exhaust.dataset.list=["eks-consumption-raw", "eks-consumption-summary", "eks-consumption-metrics","eks-creation-raw", "eks-creation-summary", "eks-creation-metrics"]
    data_exhaust.dataset.default="eks-consumption-raw"
    data_exhaust.output_format="json"
    data_exhaust.bucket="{{ .Values.global.public_container_name }}"
    cassandra.service.embedded.enable=false
    cassandra.keyspace_prefix= "{{- include "common.tplvalues.render" (dict "value" .Values.cassandra_keyspace_prefix "context" $) }}"
    device-register-controller-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      fork-join-executor {
        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 8
      }
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 1
    }
    device-register-actor-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 8
      }
      throughput = 1
    }
    device-profile-actor-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 8
      }
      throughput = 1
    }
    experiment-actor {
      type = "Dispatcher"
      executor = "fork-join-executor"
      fork-join-executor {
        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 8
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 16
      }
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 1
    }
    default-dispatcher {
      executor = "fork-join-executor"
      fork-join-executor {
        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 8
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 16
      }
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 1
    }
    #AKKA Configuration
    akka {
      actor {
        deployment {
            /druid-health-actor  {
                router = smallest-mailbox-pool
                nr-of-instances = 10
            }
            /job-service-actor {
              router = smallest-mailbox-pool
                nr-of-instances = 10
            }
            /device-register-actor {
              router = smallest-mailbox-pool
              dispatcher = device-register-actor-dispatcher
              nr-of-instances = 6
            }
            /device-profile-actor {
              router = smallest-mailbox-pool
              dispatcher = device-profile-actor-dispatcher
              nr-of-instances = 6
            }
            /client-log-actor {
              router = smallest-mailbox-pool
              nr-of-instances = 4
            }
            /experiment-actor {
              router = smallest-mailbox-pool
              nr-of-instances = 2
            }
        }
      }
    }
    #Netty Configuration
    play.server {
      # The server provider class name
      provider = "play.core.server.NettyServerProvider"
      netty {
        # The number of event loop threads. 0 means let Netty decide, which by default will select 2 times the number of
        # available processors.
        eventLoopThreads = 30
        # Whether the Netty wire should be logged
        log.wire = true
        # The transport to use, either jdk or native.
        # Native socket transport has higher performance and produces less garbage but are only available on linux
        transport = "native"
      }
    }
    play.modules.enabled+="modules.ActorInjector"
    play.filters.enabled+="filter.RequestInterceptor"
    play.filters.enabled+="play.filters.hosts.AllowedHostsFilter"
    play.filters.disabled+="play.filters.csrf.CSRFFilter"
    # Secret key
    # ~~~~~
    # The secret key is used to secure cryptographics functions.
    # If you deploy your application to several instances be sure to use the same key!
    play.http.secret.key="{{ .Values.dp_play_http_secret_key}}"
    play.filters.hosts {
       # A list of valid hosts (e.g. "example.com") or suffixes of valid hosts (e.g. ".example.com")
       # Note that ".example.com" will match example.com and any subdomain of example.com, with or without a trailing dot.
       # "." matches all domains, and "" matches an empty or nonexistent host.
      allowed = ["."]
    }
    # body parser
    play.http.parser.maxMemoryBuffer=10M
    default.consumption.app.id="no_value"
    default.channel.id="{{ .Values.channel }}"
    default.creation.app.id="no_value"
    postgres.db="{{ .Values.postgres_db }}"
    postgres.url="jdbc:postgresql://{{ .Values.global.postgresql.host }}:{{ .Values.global.postgresql.port }}/"
    postgres.user="{{ .Values.global.postgresql.postgresqlUsername }}"
    postgres.pass="{{ .Values.global.postgresql.postgresqlPassword }}"
    postgres.table_name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_name "context" $) }}"
    postgres.table.geo_location_city.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_geo_location_city_name "context" $) }}"
    postgres.table.geo_location_city_ipv4.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_geo_location_city_ipv4_name "context" $) }}"
    postgres.table.report_config.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_report_config_name "context" $) }}"
    postgres.table.job_request.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_job_request_name "context" $) }}"
    postgres.table.experiment_definition.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_experiment_definition_name "context" $) }}"
    postgres.table.dataset_metadata.name="{{- include "common.tplvalues.render" (dict "value" .Values.postgres_table_dataset_metadata_name "context" $) }}"    
    elasticsearch.service.endpoint="{{ .Values.global.elasticsearch.host }}:{{ .Values.global.elasticsearch.port }}"
    elasticsearch.index.compositesearch.name="compositesearch"
    elasticsearch.index.dialcodemetrics.name="dialcodemetrics"
    metrics.dialcodemetrics.request.limit=1000
    dataexhaust.authorization_check=true
    user.profile.url="{{ .Values.user_profile_read_url }}"
    org.search.url="{{ .Values.org_search_url }}"
    standard.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN"]
    ondemand.dataexhaust.roles=["ORG_ADMIN","REPORT_ADMIN","CONTENT_CREATOR","COURSE_MENTOR","PROGRAM_MANAGER","PROGRAM_DESIGNER"]
    dataexhaust.super.admin.channel="{{ .Values.dataexhaust_super_admin_channel }}"
    storage-service.request-signature-version="AWS4-HMAC-SHA256"
    s3service.region="ap-south-1"
    #channel exhaust configs
    channel {
      data_exhaust {
        whitelisted.consumers="{{ .Values.exhaust_api_consumer_ids }}"
        expiryMins = 30
        dataset {
          default {
            bucket="{{- include "common.tplvalues.render" (dict "value" .Values.channel_data_exhaust_bucket "context" $) }}"
            basePrefix = "data-exhaust/"
          }
          raw {
            bucket="{{- include "common.tplvalues.render" (dict "value" .Values.channel_data_exhaust_bucket "context" $) }}"
            basePrefix = "data-exhaust/"
          }
          summary {
            bucket="{{- include "common.tplvalues.render" (dict "value" .Values.channel_data_exhaust_bucket "context" $) }}"
            basePrefix = "data-exhaust/"
          }
          summary-rollup {
            bucket="{{- include "common.tplvalues.render" (dict "value" .Values.channel_data_exhaust_bucket "context" $) }}"
            basePrefix = "data-exhaust/"
          }
        }
      }
    }
    # Public exhaust configs
    public {
      data_exhaust {
        dataset {
          default {
            bucket = "{{ .Values.public_data_exhaust_bucket }}"
            basePrefix = ""
          }
        }
      }
    }
    cloud_storage_type="{{ .Values.global.sunbird_cloud_storage_provider }}"
    storage.key.config="storage_key"
    storage_key="{{ .Values.global.cloud_storage_access_key }}"
    storage.secret.config="storage_secret"
    storage_secret="{{ .Values.global.cloud_storage_secret_key }}"
    public.storage.secret.config="storage_secret_config"
    storage_secret_config="{{ .Values.global.cloud_storage_secret_key }}"

    metrics.time.interval.min=30
    cache.refresh.time.interval.min=1440
    redis.host="{{ .Values.global.redis.host }}"
    #redis.host="localhost"
    redis.port="{{ .Values.global.redis.port }}"
    #redis.port=__redis_port__
    redis.connection.max=20
    redis.connection.idle.max=20
    redis.connection.idle.min=10
    redis.connection.minEvictableIdleTimeSeconds=120
    redis.connection.timeBetweenEvictionRunsSeconds=300
    elasticsearch.host="{{ .Values.global.elasticsearch.host }}"
    elasticsearch.port="{{ .Values.global.elasticsearch.port }}"
    # experiment service settings
    elasticsearch.searchExperiment.index="experiment"
    elasticsearch.searchExperiment.fieldWeight="{\"userId\":3.0, \"deviceId\":3.0, \"url\":3.0 }"
    elasticsearch.searchExperiment.matchQueryScore=9.0
    deviceRegisterAPI.experiment.enable=false
    experimentService.redisEmptyValueExpirySeconds=86400
    redis.experimentIndex=9
    redis.deviceIndex=2
    druid.coordinator.host="{{ .Values.global.druid.host }}:{{ .Values.global.druid.port }}"
    druid.healthcheck.url="druid/coordinator/v1/loadstatus"
    device.api.enable.debug.log=true
    kafka.device.register.topic=dev.events.deviceprofile
    kafka.metrics.event.topic=dev.pipeline_metrics
    kafka.broker.list="{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
    cdn.host="https://data.{{ .Values.proxy_server_name }}/{{ .Values.public_data_exhaust_bucket }}"
    public.data_exhaust.datasets="{{ .Values.public_exhaust_datasets }}"
    public.data_exhaust.expiryMonths=6
    public.data_exhaust.max.interval.days=30
    data_exhaust.batch.limit={{ .Values.data_exhaust_batch_limit }}
    dataset.request.search.limit=10
    dataset.request.search.filters=["dataset", "requestedDate", "status", "channel"]
