application.env="{{ .Values.global.env }}"
telemetry.version="2.1"
default.parallelization="10"
spark_output_temp_dir="/data/analytics/tmp/"
lp.url="{{ .Values.lp_url }}"
service.search.url="{{ .Values.search.url }}"
service.search.path="{{ .Values.search.path }}"
spark.cassandra.connection.host="{{ .Values.global.cassandra.host }}"
cassandra.keyspace_prefix= "{{ include "common.tplvalues.render" ( dict "value" .Values.cassandra_keyspace_prefix "context" $ ) }}"
cassandra.hierarchy_store_prefix="{{ include "common.tplvalues.render" ( dict "value" .Values.cassandra_hierarchy_store_prefix "context" $ ) }}"
cloud_storage_endpoint_with_protocol= "{{- include "common.tplvalues.render" (dict "value" .Values.cloud_storage_endpoint "context" $) }}"
storage.key.config="{{ .Values.global.sunbird_cloud_storage_provider | default "" }}_storage_key" # azure or gcloud
storage.secret.config="{{ .Values.global.sunbird_cloud_storage_provider | default "" }}_storage_secret" # azure or gcloud
reports.storage.key.config="reports_storage_key"
reports.storage.secret.config="reports_storage_secret"
reports_storage_key="{{ .Values.global.cloud_storage_access_key }}"
reports_storage_secret="{{ .Values.global.cloud_storage_secret_key }}"
azure_storage_key="{{ .Values.global.cloud_storage_access_key }}"
azure_storage_secret="{{ .Values.global.cloud_storage_secret_key }}"
gcloud_storage_key="{{ .Values.global.cloud_storage_access_key }}"
gcloud_storage_secret="{{ .Values.global.cloud_storage_secret_key }}"
cloud_storage_type="{{ .Values.global.sunbird_cloud_storage_provider | default "" }}" # gcloud
lp.contentmodel.versionkey=""


# Joblog Kafka appender config for cluster execution
log.appender.kafka.enable="false"
log.appender.kafka.broker_host="{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
log.appender.kafka.topic="{{ .Values.global.env }}.druid.events.log"

# Kafka connection configuration
kafka.consumer.brokerlist="{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
kafka.consumer.topic="{{ .Values.global.env }}.analytics.job_queue"
no_of_jobs=42


# Spark Driver
spark.driver.memory=6g

spark.memory_fraction={{ .Values.spark.memory_fraction }}
spark.storage_fraction={{ .Values.spark.storage_fraction }}
spark.driver_memory={{ .Values.spark.driver_memory }}

#Monitor Jobs

monitor {
	notification {
		webhook_url = "{{ .Values.data_exhaust_webhook_url }}"
		channel = "{{ .Values.data_exhaust_Channel }}"
		token = "{{ include "common.tplvalues.render" ( dict "value" .Values.data_exhaust_token "context" $ ) }}"
		slack = true
        name = "{{ .Values.data_exhaust_name }}"
	}
}

#App ID & Channel ID
default.consumption.app.id="no_value"
default.channel.id="in.ekstep"
default.creation.app.id="no_value"


# Media Service Type
media_service_type = {{ .Values.global.sunbird_cloud_storage_provider | default "" }}

azure_tenant=""
azure_subscription_id=""
azure_account_name=""
azure_resource_group_name=""
azure_token_client_key=""
azure_token_client_secret="="
elasticsearch.service.endpoint="http://{{ .Values.global.elasticsearch.host }}:{{ .Values.global.elasticsearch.port }}"
elasticsearch.index.compositesearch.name=""

org.search.api.url="{{ .Values.channelSearchServiceEndpoint }}"
org.search.api.key= "{{.Values.global.sunbird_admin_api_token }}"

hierarchy.search.api.url="{{ .Values.hierarchySearchServiceUrl }}"
hierarchy.search.api.path="{{ .Values.hierarchySearchServicEndpoint }}"

# Azure Media Service Config
azure {
  location = "centralindia"
  tenant = "tenant name"
  subscription_id = "subscription id"

  api {
    endpoint="Media Service API End Point"
    version = "2018-07-01"
  }

  account_name = "account name"
  resource_group_name = "Resource Group Name"

  transform {
    default = "media_transform_default"
    hls = "media_transform_hls"
  }

  stream {
    base_url = "{{ .Values.stream_base_url }}"
    endpoint_name = "default"
    protocol = "Hls"
    policy_name = "Predefined_ClearStreamingOnly"
  }

  token {
    client_key = "client key"
    client_secret = "client secret"
  }
}

## Reports - Global config
cloud.container.reports="{{ .Values.global.public_container_name }}/reports"

# course metrics container in azure
course.metrics.cassandra.sunbirdKeyspace="sunbird"
course.metrics.cassandra.sunbirdCoursesKeyspace="sunbird_courses"
course.metrics.cassandra.sunbirdHierarchyStore="{{ include "common.tplvalues.render" ( dict "value" .Values.cassandra_hierarchy_store_keyspace "context" $ ) }}"
course.metrics.cloud.objectKey=""
course.metrics.cassandra.input.consistency="ONE"
es.host="http://{{ .Values.global.elasticsearch.host }}"
es.port="9200"
es.composite.host="{{ .Values.global.elasticsearch.host }}"

# State admin user reports
# Uses azure only - course.metrics.cloud.provider
admin.metrics.cloud.objectKey=""
admin.metrics.temp.dir="/data/analytics/admin-user-reports"

#Assessment report config
es.scroll.size = 1000

#BestScore or Latst Updated Score
assessment.metrics.bestscore.report=true
assessment.metrics.supported.contenttype="SelfAssess"
assessment.metrics.supported.primaryCategories="{{ .Values.assessment_metric_primary_category }}"
spark.sql.caseSensitive=true

# content rating configurations

druid.sql.host="http://{{ .Values.global.druid.brokerhost }}:{{ .Values.global.druid.brokerport }}/druid/v2/sql/"
druid.unique.content.query="{\"query\":\"SELECT DISTINCT \\\"object_id\\\" AS \\\"Id\\\"\\nFROM \\\"druid\\\".\\\"summary-events\\\" WHERE \\\"__time\\\"  BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\"}"
druid.content.rating.query="{\"query\":\"SELECT \\\"object_id\\\" AS contentId, COUNT(*) AS \\\"totalRatingsCount\\\", SUM(edata_rating) AS \\\"Total Ratings\\\", SUM(edata_rating)/COUNT(*) AS \\\"averageRating\\\" FROM \\\"druid\\\".\\\"telemetry-feedback-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' AND \\\"edata_rating\\\">0 GROUP BY \\\"object_id\\\"\"}"
druid.content.consumption.query="{\"query\":\"SELECT COUNT(*) as \\\"play_sessions_count\\\", object_id as \\\"contentId\\\", SUM(total_time_spent) as \\\"total_time_spent\\\", dimensions_pdata_id, object_id\\nFROM \\\"summary-events\\\"\\nWHERE \\\"dimensions_mode\\\" = 'play' AND \\\"dimensions_type\\\" ='content' AND \\\"dimensions_pdata_pid\\\" != 'creation-portal' \\nGROUP BY object_id, dimensions_pdata_id\"}"
lp.system.update.base.url="{{ .Values.lp_url }}/system/v3/content/update"


#Experiment Configuration

user.search.api.url="{{ .Values.sunbird_userorg_service_url }}/private/user/v1/search"
user.search.limit="10000"

# pipeline auditing
druid.pipeline_metrics.audit.query="{\"query\":\"SELECT \\\"job-name\\\", SUM(\\\"success-message-count\\\") AS \\\"success-message-count\\\", SUM(\\\"failed-message-count\\\") AS \\\"failed-message-count\\\", SUM(\\\"duplicate-event-count\\\") AS \\\"duplicate-event-count\\\", SUM(\\\"batch-success-count\\\") AS \\\"batch-success-count\\\", SUM(\\\"batch-error-count\\\") AS \\\"batch-error-count\\\", SUM(\\\"primary-route-success-count\\\") AS \\\"primary-route-success-count\\\", SUM(\\\"secondary-route-success-count\\\") AS \\\"secondary-route-success-count\\\" FROM \\\"druid\\\".\\\"pipeline-metrics\\\" WHERE \\\"job-name\\\" IN (%s) AND \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s' GROUP BY \\\"job-name\\\" \"}"
druid.telemetryDatasource.count.query="{ \"query\": \"SELECT COUNT(*) AS \\\"total\\\" FROM \\\"druid\\\".\\\"telemetry-events\\\" WHERE TIME_FORMAT(MILLIS_TO_TIMESTAMP(\\\"syncts\\\"), 'yyyy-MM-dd HH:mm:ss.SSS', 'Asia/Kolkata') BETWEEN TIMESTAMP '%s' AND '%s' AND  \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\" }"
druid.summaryDatasource.count.query="{\"query\": \"SELECT COUNT(*) AS \\\"total\\\" FROM \\\"druid\\\".\\\"summary-events\\\" WHERE \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\" }"

#Pipeline Audit Jobs

pipeline_audit {
	notification {
		webhook_url = "{{ .Values.data_exhaust_webhook_url }}"
		channel = "{{ .Values.data_exhaust_Channel }}"
		token = "{{ .Values.data_exhaust_token }}"
		slack = true
		name = "Pipeline Audit"
	}
}

#Druid Query Processor

druid = {
	hosts = "http://{{ .Values.global.druid.brokerhost }}:{{ .Values.global.druid.brokerport }}"
	secure = false
	url = "/druid/v2/"
	datasource = "telemetry-events"
	response-parsing-timeout = 300000
        client-backend = "com.ing.wbaa.druid.client.DruidAdvancedHttpClient"
        client-config = {
           druid-advanced-http-client ={
           queue-size = 32768
           queue-overflow-strategy = "Backpressure"
           query-retries = 5
           query-retry-delay = 10 ms
           host-connection-pool = {
             max-connections = 32
             min-connections = 0
             max-open-requests = 128
             max-connection-lifetime = 20 min
             idle-timeout = 15 min
          client = {
            # The time after which an idle connection will be automatically closed.
            # Set to `infinite` to completely disable idle timeouts.
            idle-timeout = 10 min
           parsing.max-chunk-size = 10m
        }
        }
    }

  }
}
druid.rollup.host="{{ .Values.global.druid.brokerhost }}"
druid.rollup.port=8082
druid.query.wait.time.mins=10
druid.report.upload.wait.time.mins=10
druid.scan.batch.size=100
druid.scan.batch.bytes=2000000
druid.query.batch.buffer=500000


// Metric event config
metric.producer.id="pipeline.monitoring"
metric.producer.pid="dataproduct.metrics"
push.metrics.kafka=true
metric.kafka.broker="{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
metric.kafka.topic="{{ .Values.global.env }}.prom.monitoring.metrics"

//Postgres Config
postgres.db="analytics"
postgres.url="jdbc:postgresql://{{ .Values.global.postgresql.host }}:{{ .Values.global.postgresql.port }}/"
postgres.user="{{ .Values.global.postgresql.postgresqlUsername }}"
postgres.pass="{{ .Values.global.postgresql.postgresqlPassword }}"
postgres.program.table="program"
postgres.nomination.table="nomination"
postgres.usertable="\"V_User\""
postgres.org.table="\"V_User_Org\""

druid.ingestion.path="/druid/indexer/v1/task"
druid.segment.path="/druid/coordinator/v1/metadata/datasources/"
druid.deletesegment.path="/druid/coordinator/v1/datasources/"

postgres.druid.db="druid"
postgres.druid.url="jdbc:postgresql://{{ .Values.global.postgresql.host }}:{{ .Values.global.postgresql.port }}/"
postgres.druid.user="{{ .Values.global.postgresql.postgresqlUsername }}"
postgres.druid.pass="{{ .Values.global.postgresql.postgresqlPassword }}"


location.search.url="https://{{ .Values.location_search_url}}/v1/location/search"
location.search.token="Bearer {{ .Values.global.sunbird_admin_api_token }}"
location.search.request="{\"request\": {\"filters\": {\"type\" :[\"state\",\"district\"]},\"limit\" : 10000}}"

druid.state.lookup.url = "http://{{ .Values.global.druid.coordinatorhost }}:{{ .Values.global.druid.coordinatorport }}/druid/coordinator/v1/lookups/config/__default/stateSlugLookup"

sunbird_encryption_key="{{ .Values.global.random_string  }}"

dcedialcode.filename="DCE_dialcode_data.csv"
etbdialcode.filename="ETB_dialcode_data.csv"
dcetextbook.filename="DCE_textbook_data.csv"
etbtextbook.filename="ETB_textbook_data.csv"
etb.dialcode.druid.length={{ .Values.etb_dialcode_list_druid_length }}


druid.report.default.storage={{ .Values.global.sunbird_cloud_storage_provider | default "" }}
druid.report.date.format="yyyy-MM-dd"
druid.report.default.container="report-verification"

## Collection Exhaust Jobs Configuration -- Start ##

sunbird.user.keyspace="{{ .Values.user_table_keyspace }}"
sunbird.courses.keyspace="{{ .Values.course_keyspace }}"
sunbird.content.hierarchy.keyspace="{{ include "common.tplvalues.render" ( dict "value" .Values.cassandra_hierarchy_store_keyspace "context" $ ) }}"
sunbird.user.cluster.host="{{ .Values.core_cassandra_host | default  .Values.global.cassandra.host }}"
sunbird.courses.cluster.host="{{ .Values.core_cassandra_host | default  .Values.global.cassandra.host }}"
sunbird.content.cluster.host="{{ .Values.core_cassandra_host | default  .Values.global.cassandra.host }}"
sunbird.report.cluster.host="{{ .Values.report_cassandra_host | default  .Values.global.cassandra.host }}"
sunbird.user.report.keyspace="{{ .Values.report_user_table_keyspace }}"
collection.exhaust.store.prefix=""
postgres.table.job_request="{{ .Values.global.env }}_job_request"
postgres.table.dataset_metadata="{{ .Values.global.env }}.dataset_metadata_table"

## Collection Exhaust Jobs Configuration -- End ##

## Exhaust throttling variables
exhaust.batches.limit.per.channel="{{ .Values.exhaust_batches_limit_per_channel }}"
exhaust.file.size.limit.per.channel="{{ .Values.exhaust_file_size_limit_bytes_per_channel | toString }}"

exhaust.parallel.batch.load.limit="{{ .Values.exhaust_parallel_batch_load_limit }}"
exhaust.user.parallelism="{{ .Values.exhaust_user_parallelism }}"

data_exhaust.batch.limit.per.request="{{ .Values.data_exhaust_batch_limit_per_request }}"



//START of UCI Postgres Config

uci.conversation.postgres.db=""
uci.conversation.postgres.url="jdbc:postgresql://{{ .Values.global.postgresql.host }} :{{ .Values.global.postgresql.port }}/"

uci.fushionauth.postgres.db=""
uci.fushionauth.postgres.url="jdbc:postgresql://{{ .Values.global.postgresql.host }} :{{ .Values.global.postgresql.port }}/"

uci.postgres.table.conversation=""
uci.postgres.table.user=""
uci.postgres.table.user_registration=""
uci.postgres.table.identities=""

uci.conversation.postgres.user=""
uci.conversation.postgres.pass=""

uci.fushionauth.postgres.user=""
uci.fushionauth.postgres.pass=""

uci.exhaust.store.prefix=""
uci.encryption.secret="{{ .Values.uci_encryption_secret_key }}"

// END OF UCI Related Job Configs