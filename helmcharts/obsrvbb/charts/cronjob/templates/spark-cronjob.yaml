apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-script-cronjob-user-cache
spec:
  schedule: "0 */12 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-cron
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -- /data/analytics/content-snapshot/scripts/run-script.sh USER_CACHE_INDEXER {{ .Values.user_id }} {{ .Values.date }} {{ .Values.populate_anonymous_user }} {{ .Values.refresh_data }}
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-script-cronjob-druid-content
spec:
  schedule: "0 */12 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-cron
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -- /data/analytics/content-snapshot/scripts/run-script.sh DRUID_CONTENT_INDEXER {{ .Values.user_id }} {{ .Values.date }} {{ .Values.populate_anonymous_user }} {{ .Values.refresh_data }}
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-script-cronjob-content-cache
spec:
  schedule: "0 */12 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-cron
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -- /data/analytics/content-snapshot/scripts/run-script.sh CONTENT_CACHE_INDEXER {{ .Values.user_id }} {{ .Values.date }} {{ .Values.populate_anonymous_user }} {{ .Values.refresh_data }}
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---

apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-script-cronjob-dialcode-cache
spec:
  schedule: "0 */12 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark-cron
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -- /data/analytics/content-snapshot/scripts/run-script.sh DIALCODE_CACHE_INDEXER {{ .Values.user_id }} {{ .Values.date }} {{ .Values.populate_anonymous_user }} {{ .Values.refresh_data }}
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-wfs-job
spec:
  schedule: "30 0 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: daily-cron-jobs-for-wfs
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh wfs"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-monitor-job
spec:
  schedule: "0 3 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: daily-cron-jobs-for-monitor-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh monitor-job-summ"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: submit-jobs
spec:
  schedule: "35 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: daily-cron-jobs-using-submit-all-jobs
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "export PATH="/usr/local/rvm/bin:$PATH" && export GEM_HOME=/.gem/ruby/2.6.0 && ruby /data/analytics/scripts/submit-all-jobs.rb"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: start-jobmanager
spec:
  schedule: "30 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: start-jobmanager-cron-jobs
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/start-jobmanager.sh"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: course-batch-status-updater
spec:
  schedule: "0 */12 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: course-batch-status-updater-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh course-batch-status-updater"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa


---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-admin-user-reports-job
spec:
  schedule: "30 3,14 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: admin-user-reports-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh admin-user-reports"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-admin-geo-report-job
spec:
  schedule: "0 4 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: admin-geo-reports-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh admin-geo-reports"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-progress-exhaust
spec:
  schedule: "0 8 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: progress-exhaust-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh progress-exhaust"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-response-exhaust
spec:
  schedule: "0 9 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: response-exhaust-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh response-exhaust"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-cassandra-migration
spec:
  schedule: "15 19 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cassandra-migration-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh cassandra-migration"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-userinfo-exhaust
spec:
  schedule: "0 10 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: userinfo-exhaust-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh userinfo-exhaust"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: run-collection-summary
spec:
  schedule: "30 9 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: collection-summary-cron-job
            image: bitnami/kubectl:1.28.4
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "bash -x /data/analytics/scripts/run-job.sh collection-summary-report-v2"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa


---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: druid-report-processor
  labels:
    app: druid-report-processor
spec:
  schedule: "30 3 * * *"
  concurrencyPolicy: Allow
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: druid-report-processor
            image: bitnami/kubectl:1.28.4
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - |
              SPARK_POD=spark-master-0
              kubectl exec -it $SPARK_POD -- bash -c "python3 /data/analytics/scripts/druid-report-processor.py"
            env:
            - name: SPARK_HOME
              value: "/data/analytics/spark-3.1.3-bin-hadoop2.7"
            - name: MODELS_HOME
              value: "/data/analytics/models-2.0"
            - name: DP_LOGS
              value: "/data/analytics/logs/data-products"
            - name: KAFKA_HOME
              value: "/opt/bitnami/kafka_2.12-2.8.0"
            - name: KAFKA_TOPIC
              value: "{{ .Values.analytics_job_queue_topic }}"
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "kafka:9092"
            - name: KAFKA_CONSUMER_GROUP
              value: "druid-report-processor-group"
          restartPolicy: OnFailure
          serviceAccountName: spark-cronjob-sa