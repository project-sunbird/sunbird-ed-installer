nameOverride: "flink"
fullnameOverride: "flink"

replicaCount: 1

image:
  repository: sunbirded.azurecr.io/data-pipeline
  tag: release-8.0.0_RC1_c30fd96_5
  pullPolicy: IfNotPresent
  pullSecrets: []  

podAnnotations: {}

podSecurityContext:
  # runAsNonRoot: true
  runAsUser: 0
  fsGroup: 0

securityContext: {}
  # readOnlyRootFilesystem: false
  # capabilities:
  #   drop:
  #   - ALL

service:
  type: ClusterIP
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081

ingress: {}

resources:
  requests:
    cpu: 100m
    memory: 500M
  limits:
    cpu: 1
    memory: 2048Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80


nodeSelector: {}
tolerations: []
affinity: {}

configmap:
  enabled: false
  mountPath: /config

serviceAccount:
  create: true
  name: ""

serviceMonitor:
  enabled: false
  interval: 30s
  scrapeTimeout: 10s
  labels: {} # additional labels e.g. release: prometheus
  honorLabels: true
  jobLabel: "app.kubernetes.io/name"

initContainers: {}
sidecars: {}

opa:
  enabled: false


jobmanager:
  rpc_port: 6123
  blob_port: 6124
  query_port: 6125
  ui_port: 8081
  prom_port: 9250
  heap_memory: 1024

rest_port: 80
resttcp_port: 8081

taskmanager:
  prom_port: 9251
  rpc_port: 6122
  heap_memory: 1024
  replicas: 1
  cpu_requests: 0.3

### base-config related vars
oci_flink_s3_storage_endpoint: "s3/storage/endpoint"
postgres_max_connections: 2
flink_container_name: flink-check-points-store
checkpoint_interval: 60000
checkpoint_pause_between_seconds: 5000
checkpoint_compression_enabled: true
restart_attempts: 3
restart_delay: 30000 # in milli-seconds
producer_max_request_size: 1572864

### QuestionSet Publish Config
master_category_validation_enabled: "Yes"

inquiry_assessment_publish_kafka_topic_name: "{{ .Values.global.env }}.assessment.publish.request"
inquiry_assessment_post_publish_kafka_topic_name: "{{ .Values.global.env }}.assessment.postpublish.request"
inquiry_assessment_publish_group: "{{ .Values.global.env }}-questionset-publish-group"
question_keyspace_name: "{{ .Values.global.env }}_question_store"
hierarchy_keyspace_name: "{{ .Values.global.env }}_hierarchy_store"
kp_print_service_base_url: "http://print:5000"
# CNAME Config
cloudstorage_relative_path_prefix: "CONTENT_STORAGE_BASE_PATH"
cloudstorage_base_path: "https://{{ .Values.global.object_storage_endpoint }}"
cloudstorage_metadata_list: '["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl"]'
valid_cloudstorage_base_urls: '["https://{{ .Values.global.object_storage_endpoint }}"]'
cloudstorage_replace_absolute_path: false
### QuestionSet RePublish Job Config
inquiry_assessment_republish_kafka_topic_name: "{{ .Values.global.env }}.assessment.republish.request"
inquiry_assessment_republish_group: "{{ .Values.global.env }}-questionset-republish-group"
### user_pii Job Config
user_pii_updater_kafka_topic_name: "{{ .Values.global.env }}.delete.user"
user_pii_updater_group: "{{ .Values.global.env }}-user-pii-updater-group"
user_pii_target_object_types: '{
  "Question": ["1.0", "1.1"],
  "QuestionSet": ["1.0", "1.1"],
  "Asset": ["1.0"],
  "Content": ["1.0"],
  "Collection": ["1.0"]
}'
user_pii_replacement_value: "Deleted User"
user_org_service_base_url: "http://userorg-service:9000"
email_notification_subject: "User Account Deletion Notification"
email_notification_regards: "Team"


log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = INFO
  rootLogger.appenderRef.console.ref = ConsoleAppender

  # Uncomment this if you want to _only_ change Flink's logging
  #logger.flink.name = org.apache.flink
  #logger.flink.level = INFO

  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = ERROR
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = ERROR
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = ERROR
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = ERROR

  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF

base_config: |
  kafka {
      broker-servers = "{{ .Values.global.kafka.host }}:{{ .Values.global.kafka.port }}"
      zookeeper = "{{ .Values.global.zookeeper.host }}:{{ .Values.global.zookeeper.port }}"
      producer {
        max-request-size = {{ .Values.producer_max_request_size }}
      }
      input {
      user_pii_topic = "{{ .Values.global.env }}.delete.user.job.request"
      ownership_transfer_topic = "{{ .Values.global.env }}.user.ownership.transfer"
    }
    groupId = "{{ .Values.global.env }}-user-pii-data-updater-group"
  }

  job {
    env = "{{ .Values.global.env }}"
    enable.distributed.checkpointing = true

    {{- if eq "oci" .Values.global.checkpoint_store_type }}
    statebackend {
      base.url = "s3://{{ .Values.global.public_container_name }}/checkpoint"
    }

    {{- else if eq "azure" .Values.global.checkpoint_store_type }}
    statebackend {
      base.url = "wasbs://{{ .Values.global.public_container_name }}@{{ .Values.global.object_storage_endpoint }}/checkpoint"
    }

    {{- else if eq "gcs" .Values.global.checkpoint_store_type }}
    statebackend {
      base.url = "gs://{{ .Values.global.public_container_name }}/checkpoint"
    }
    {{- else }}
    # No valid checkpoint_store_type configured, skipping statebackend.
    {{- end }}
  }
  
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = {{ .Values.checkpoint_compression_enabled }}
      checkpointing.interval = {{ .Values.checkpoint_interval }}
      checkpointing.pause.between.seconds = {{ .Values.checkpoint_pause_between_seconds }}
      restart-strategy.attempts = {{ .Values.restart_attempts }}
      restart-strategy.delay = {{ .Values.restart_delay }} # in milli-seconds
    }
    redis {
      host = {{ .Values.global.redis.host }}
      port = {{ .Values.global.redis.port }}
    }
    lms-cassandra {
      host = "{{ .Values.global.cassandra.host }}"
      port = {{ .Values.global.cassandra.port }}
    }
    neo4j {
      routePath = "bolt://{{.Values.global.neo4j.host}}:{{.Values.global.neo4j.port}}"
      graph = "domain"
    }
    es {
        basePath = "{{.Values.global.elasticsearch.host}}:{{.Values.global.elasticsearch.port}}"
    }
    schema {
      basePath = "{{- include "common.tplvalues.render" (dict "value" .Values.global.schema.base_path "context" $) }}"
      supportedVersion = {
        itemset = "2.0"
      }
    }
flink_jobs:
  async-questionset-publish:
    enabled: true
    config: |+
      include file("/data/flink/conf/base-config.conf")
      kafka {
        input.topic = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_publish_kafka_topic_name "context" $) }}
        post_publish.topic = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_post_publish_kafka_topic_name "context" $) }}
        groupId = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_publish_group "context" $) }}
      }
      task {
        consumer.parallelism = 1
        parallelism = 1
        router.parallelism = 1
      }
      question {
        keyspace = {{- include "common.tplvalues.render" (dict "value" .Values.question_keyspace_name "context" $) }}
        table = "question_data"
      }
      questionset {
        keyspace = {{- include "common.tplvalues.render" (dict "value" .Values.hierarchy_keyspace_name "context" $) }}
        table = "questionset_hierarchy"
      }
      print_service.base_url = "{{ .Values.kp_print_service_base_url }}"
      #Cloud Storage Config
      cloud_storage_type: "{{ .Values.global.checkpoint_store_type }}"
      cloud_storage_key: "{{ .Values.global.cloud_storage_access_key }}"  
      cloud_storage_secret: "{{ .Values.global.cloud_storage_secret_key }}" 
      cloud_storage_endpoint: ""
      cloud_storage_container: "{{ .Values.global.public_container_name }}"

      master.category.validation.enabled ="{{ .Values.master_category_validation_enabled }}"
      cloudstorage {
        metadata.replace_absolute_path={{ .Values.cloudstorage_replace_absolute_path | default false }}
        metadata.list= {{ .Values.cloudstorage_metadata_list }}
        relative_path_prefix="{{ .Values.cloudstorage_relative_path_prefix }}"
        read_base_path="{{- include "common.tplvalues.render" (dict "value" .Values.cloudstorage_base_path "context" $) }}"
        write_base_path= {{- include "common.tplvalues.render" (dict "value" .Values.valid_cloudstorage_base_urls "context" $) }}
      }
     
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
    job_classname: org.sunbird.job.questionset.task.QuestionSetPublishStreamTask
  
  questionset-republish:
    enabled: true
    config: |+
      include file("/data/flink/conf/base-config.conf")
      kafka {
        input.topic = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_republish_kafka_topic_name "context" $) }}
        post_publish.topic = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_post_publish_kafka_topic_name "context" $) }}
        groupId = {{- include "common.tplvalues.render" (dict "value" .Values.inquiry_assessment_republish_group "context" $) }}
      }
      task {
        consumer.parallelism = 1
        parallelism = 1
        router.parallelism = 1
      }
      question {
        keyspace = {{- include "common.tplvalues.render" (dict "value" .Values.question_keyspace_name "context" $) }}
        table = "question_data"
      }
      questionset {
        keyspace = {{- include "common.tplvalues.render" (dict "value" .Values.hierarchy_keyspace_name "context" $) }}
        table = "questionset_hierarchy"
      }
      print_service.base_url = "{{ .Values.kp_print_service_base_url }}"
      #Cloud Storage Config
      cloud_storage_type: "{{ .Values.global.checkpoint_store_type }}"
      cloud_storage_key: "{{ .Values.global.cloud_storage_access_key }}"
      cloud_storage_secret: "{{ .Values.global.cloud_storage_secret_key }}"
      cloud_storage_endpoint: ""
      cloud_storage_container: "{{ .Values.global.public_container_name }}"
      
      master.category.validation.enabled ="{{ .Values.master_category_validation_enabled }}"
      cloudstorage {
        metadata.replace_absolute_path={{ .Values.cloudstorage_replace_absolute_path | default false }}
        metadata.list={{ .Values.cloudstorage_metadata_list }}
        relative_path_prefix="{{ .Values.cloudstorage_relative_path_prefix }}"
        read_base_path="{{- include "common.tplvalues.render" (dict "value" .Values.cloudstorage_base_path "context" $) }}"
        write_base_path= {{- include "common.tplvalues.render" (dict "value" .Values.valid_cloudstorage_base_urls "context" $) }}
      }
     
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
    job_classname: org.sunbird.job.questionset.republish.task.QuestionSetRePublishStreamTask  
  
  user-pii-data-updater:
    enabled: true
    config: |+
      include file("/data/flink/conf/base-config.conf")
      kafka {
        input.topic = "{{- include "common.tplvalues.render" (dict "value" .Values.user_pii_updater_kafka_topic_name "context" $) }}"
        groupId = "{{- include "common.tplvalues.render" (dict "value" .Values.user_pii_updater_group "context" $) }}"
      }
      task {
        consumer.parallelism = 1
        parallelism = 1
        router.parallelism = 1
      }
      target_object_types={{ .Values.user_pii_target_object_types }}
      user_pii_replacement_value="{{ .Values.user_pii_replacement_value }}"
      admin_email_notification_enable=true
      userorg_service_base_url="{{- include "common.tplvalues.render" (dict "value" .Values.user_org_service_base_url "context" $) }}"
      notification {
        email {
          subject: "{{ .Values.email_notification_subject }}",
          regards: "{{ .Values.email_notification_regards }}"
        }
      }
    flink-conf: |+
      jobmanager.memory.flink.size: 1024m
      taskmanager.memory.flink.size: 1024m
      taskmanager.numberOfTaskSlots: 1
      parallelism.default: 1
      jobmanager.execution.failover-strategy: region
      taskmanager.memory.network.fraction: 0.1
    job_classname: org.sunbird.job.user.pii.updater.task.UserPiiUpdaterStreamTask
